{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='/Users/ekinokos2/datasets/MNIST', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='/Users/ekinokos2/datasets/MNIST', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Logistic loss\n",
    "\n",
    "# Define different learning rates to try\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Define different learning rate decay values (for AdaGrad)\n",
    "lr_decay = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Define three different values for adaptivity momentum (for Adam)\n",
    "momentum = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Define three different values for gradient momentum (for Adam)\n",
    "grad_momentum = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Kaiming Initialization\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: SGD, Learning Rate: 0.001, Accuracy: 94.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, optimizer \u001b[39min\u001b[39;00m optimizers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     model\u001b[39m.\u001b[39mapply(weights_init)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     training_losses, accuracy \u001b[39m=\u001b[39m train_and_test(model, optimizer, criterion, train_loader, test_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49mlr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOptimizer: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Learning Rate: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, lr, accuracy))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     results[(name, lr)] \u001b[39m=\u001b[39m (training_losses, accuracy)\n",
      "\u001b[1;32m/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/cse543/hw1.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_and_test(model, optimizer, criterion, train_loader, test_loader, epochs, learning_rate):\n",
    "    model.train()\n",
    "    optimizer.param_groups[0]['lr'] = learning_rate  # Set the learning rate\n",
    "    training_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        training_losses.append(running_loss / len(train_loader))\n",
    "    accuracy = test(model, test_loader)\n",
    "    return training_losses, accuracy\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "    model = LeNet()\n",
    "    optimizers = {\n",
    "    \"SGD\": optim.SGD(model.parameters(), lr=lr),\n",
    "    \"BatchGD\": optim.SGD(model.parameters(), lr=lr, momentum=0.0, nesterov=False),\n",
    "    \"AdaGrad\": optim.Adagrad(model.parameters(), lr=lr),\n",
    "    \"Adam\": optim.Adam(model.parameters(), lr=lr)\n",
    "    }\n",
    "    for name, optimizer in optimizers.items():\n",
    "        model.apply(weights_init)\n",
    "        training_losses, accuracy = train_and_test(model, optimizer, criterion, train_loader, test_loader, epochs=10, learning_rate=lr)\n",
    "        print(\"Optimizer: {}, Learning Rate: {}, Accuracy: {}\".format(name, lr, accuracy))\n",
    "        results[(name, lr)] = (training_losses, accuracy)\n",
    "\n",
    "\n",
    "# Plot the training losses for different learning rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "for lr in learning_rates:\n",
    "    losses, _ = results[lr]\n",
    "    plt.plot(range(1, 11), losses, label=f'LR={lr}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss for Different Learning Rates')\n",
    "plt.show()\n",
    "\n",
    "# Print accuracy results for different learning rates\n",
    "for lr, (_, accuracy) in results.items():\n",
    "    print(f'Accuracy for LR={lr}: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.Adagrad?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
